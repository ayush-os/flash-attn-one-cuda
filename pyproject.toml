[project]
name = "flash-attn"
version = "0.1.0"
description = "Flash Attention CUDA implementation"
readme = "README.md"
authors = [
    { name = "ayush", email = "99374392+ayush-os@users.noreply.github.com" }
]
requires-python = ">=3.12"
dependencies = [
    "torch",
    "numpy",
    "setuptools",
]

[tool.setuptools]
package-dir = {"" = "src"}

[build-system]
requires = ["setuptools", "torch", "numpy"]
build-backend = "setuptools.build_meta"
